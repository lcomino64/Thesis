\chapter[Literature Review]{Literature Review}
\label{Chap:Literature Review}
\section{Computer Networking}
Communication between devices on a network is enabled through a standardized set of protocols and interfaces, allowing devices to communicate regardless of any underlying hardware or architecture. The TCP/IP protocol suite is most common set of protocols which organises these protocols into distinct layers to handle different aspects of network communication. This layered approach allows for modular development and maintenance of network functionality \cite{Forouzan2021}\cite{SahaRony2016}\cite{KuroseRoss2021}. 

\begin{wrapfigure}[16]{r}{0.38\textwidth}
    \caption{TCP/IP Model \cite{Forouzan2021}}
    \label{TCPIPStack}
    \centering
    \includegraphics[width=0.35\textwidth]{Chapter2/Figures/tcp-ip-stack.png}
\end{wrapfigure}

There are five primary layers. Figure \ref{TCPIPStack}, shows how these layers are organised.

\textbf{4. Application Layer}:

Provides network services directly to end-users.
Protocols include HTTP, FTP, SSH, and DNS.

\textbf{3. Transport Layer}:

Manages end-to-end communication. Implements TCP and UDP transfer protocols. Handles flow control and errors such as missing packets at a high-level.

\textbf{2. Network Layer}:

Routes packets between networks using IP addressing. Handles packet fragmentation and reassembly. Makes routing decisions and uses best-effort delivery without guarantees.

\textbf{1. Link Layer}:

Manages direct communication between devices on the same network. Handles physical addressing via MAC. Also provides error detection. Protocols include Ethernet, Wi-Fi, and PPP.

\textbf{0. Physical Layer}:

Responsible for transmitting raw bits over physical media (copper wire, fiber optic or radio). Electrical signal methods are handled here, such as voltage levels for HIGH and LOW bits, and timing.

In the coming sections, we will detail each of the relevant layers in the project and the important protocols in each. It should be noted that the technologies and protocols spanning the TCP/IP stack are vast and extensively documented; the following sections serve only as a rudimentary summary of the concepts most pertinent to this project's implementation.

\subsection{Transport Layer}
The transport layer provides end-to-end communication services between applications running on different hosts. At its core, this layer segments application data into smaller units called packets - formatted blocks of data that include both control information and user data. A packet consists of a header containing protocol-specific control information, and a payload containing the actual data being transmitted \cite{SahaRony2016}.

In terms of how these packets are handled, we have two over-arching protocols which distinctly handle the two most prominent types of data transfer. For lossless transmission and reliability at the cost of a performance overhead, we have the Transmission Communication Protocol, TCP. For connectionless transfers where only data throughput is prioritised, we have User Datagram Protocol. 
The idea behind each protocol is best demonstrated in the contrast between how packets are formatted, a visual representation of which can be seen in figure \ref{TCPvsUDP}:
\begin{figure}[!ht]
    \caption{TCP and UDP headers \cite{SahaRony2016}}
    \label{TCPvsUDP}
    \centering
    \includegraphics[width=1.0\textwidth]{Chapter2/Figures/TCPvsUDP.png}
\end{figure}

\subsubsection{TCP vs UDP}
Before a TCP data transmission begins, connection through a three-way handshake between sender and receiver is established. This connection maintains state information throughout the transmission process, enabling tracking of every packet sent and received. For reliability, each packet is assigned a sequence number, allowing the receiver to reconstruct the data stream in the correct order and detect any missing packets. When packets are lost or corrupted, TCP's acknowledgment system automatically triggers retransmission \cite{KuroseRoss2021}.

UDP takes a  different approach, operating as a connectionless protocol with minimal overhead. Unlike TCP, UDP begins transmitting data immediately without establishing a connection or maintaining state information \cite{KuroseRoss2021}. It simply sends packets (called datagrams in UDP) to the target destination without guaranteeing their arrival or ordering. This means UDP cannot guarantee reliable delivery, but it achieves lower latency and higher throughput compared to TCP \cite{KuroseRoss2021}.

Applications requiring guaranteed data delivery and correct ordering, such as file transfers or database operations, typically use TCP despite its performance overhead. Conversely, applications where speed is crucial and occasional packet loss is acceptable, such as real-time video streaming or online gaming, often choose UDP \cite{Forouzan2021}.

\subsubsection{Sockets and Ports}
Seen in Figure \ref{TCPvsUDP}, both headers for UDP and TCP contain source/destination port fields. The Transport layer implements the concept of ports and sockets in order to multiplex network connections on a single device \cite{Forouzan2021}. A socket, representing the combination of an IP address and 16-bit port number, creates a unique communication endpoint, which is an abstraction that allows for multiple applications to communicate simultaneously without conflict.

\subsection{Network Layer}
The Network layer handles the routing and addressing of packets between networks using the Internet Protocol (IP). For devices on a local network switch, as in this project, the Network layer's role is relatively straightforward - it primarily handles packet routing through IP and basic packet fragmentation when necessary \cite{KuroseRoss2021}. Since all project devices exist within the same network segment connected via an ethernet switch, there is no complex routing involved; packets are simply addressed between known IP addresses on the local subnet, \ie, no device will be separated by more than two links. The layer does, however, still maintain its responsibility for packet fragmentation and reassembly, ensuring data can be transmitted within the Maximum Transmission Unit (MTU) size of the local network \cite{Forouzan2021}.

\newpage

\subsection{Data Link Layer, Ethernet MAC}
\label{section:Link}
The Data Link layer, implemented through the Ethernet Media Access Control (MAC), manages direct communication between devices on the same network segment. The MAC sublayer handles addressing via unique 48-bit MAC addresses, frame formation, and error detection through Cyclic Redundancy Checks (CRC). Additionally, there is another field which specifies the ``EtherType". EtherType is a 16-bit identifier that indicates which protocol is encapsulated in the frame's payload (e.g., 0x0800 for IPv4, 0x0806 for ARP, 0x86DD for IPv6). This field enables the receiver to correctly demultiplex incoming frames to the appropriate higher-layer protocol handler in the networking stack. Figure \ref{Ethernet2Frame} illustrates the structure of an Ethernet II frame. This frame type is most common, contains all the information needed to transfer data between ethernet MACs.

\begin{figure}[!ht]
    \begin{center}
        \includegraphics[width=1.0\textwidth]{./Chapter2/Figures/Ethernet_Type_II_Frame_format.png}
        \caption{The Common Ethernet Frame Format, Type II Frame Buffer \cite{Ethernet_Wikipedia_2024}}
    \label{Ethernet2Frame}
    \end{center}
\end{figure}

The MAC also implements carrier sense multiple access with collision detection (CSMA/CD) for shared medium arbitration, though this function is less utilised in modern switched networks where full-duplex operation is standard \cite{KuroseRoss2021}.

Ethernet MAC implementations on FPGAs typically use vendor-provided IP cores that implement the MAC layer in FPGA fabric. Xilinx offers several ``soft" MAC implementations through Vivado, each with different trade-offs. The Tri-Mode Ethernet MAC (TEMAC) IP core supports 10/100/1000 Mbps operation with full features \cite{xilinx_tri_mode}. The AXI 1G/2.5G Ethernet Subsystem IP provides the same MAC functionality but with an AXI4-Stream interface for easier system integration \cite{xilinx_axi_eth}. Both these implementations can get exhaustive with the consumption of FPGA fabric resources such as Look-Up Tables (LUTs) and Block RAM (BRAM) for packet buffering. To mitigate this, if high-speed transfers are not required, the Ethernet Lite MAC IP offers a minimal resource implementation at 10/100 Mbps \cite{xilinx_lite_mac}.

\subsection{Physical Layer, Ethernet PHY}
\label{section:PHY}
The Physical layer is responsible for the actual transmission of raw bits across the physical medium, typically through twisted-pair copper cables in modern Ethernet networks. At this layer, digital data from above is converted into electrical signals suitable for transmission.

The connection between the MAC and PHY is standardized through the Media Independent Interface (MII). This interface exists in several variants that support different speeds:
\begin{itemize}
    \item MII: The standard interface supporting 10/100 Mbps using a 4-bit data path
    \item RMII (Reduced MII): A simplified interface using a 2-bit data path to reduce pin count
    \item GMII (Gigabit MII): An enhanced interface supporting up to 1000 Mbps with an 8-bit data path
\end{itemize}

For higher throughput applications beyond 1 Gbps, interfaces such as XGMII (10 Gbps) and CGMII (100 Gbps) exist. These are often implemented through Small Form-factor Pluggable (SFP) transceivers - modular interfaces that support both optical and copper connections at high speeds. Complete lists of these ethernet interfaces are provided by the corresponding $Wikipedia$ articles: MII \cite{MII_Wikipedia_2024} and SFP \cite{SFP_Wikipedia_2024}.

To attach a basic PHY interface, the Management Data Input/Output (MDIO) provides the serial communication channel between the MAC and PHY. Beyond just configuration and status monitoring, this two-wire interface (MDC clock and MDIO data) handles the essential data exchange between the MAC and PHY, including the speed negotiation, link status, and error conditions.

In FPGA implementations, achieving maximum throughput requires consideration of several factors. Clock domain crossing between the MAC and PHY interfaces must be properly managed, along with appropriate sizing of transmit/receive buffers. Most important, however, is the processing capability of the system itself - insufficient processing speed is guaranteed to bottleneck a higher-end ethernet interface.

\subsubsection{Data Link/Physical Layer Summary}
We mentioned several different interfaces in sections: \ref{section:Link} and \ref{section:PHY}, that carry an ethernet signal from the port to the CPU. Figure \ref{EthernetPHYSystem} is a high level overview of a typical implementation:

\begin{figure}[!ht]
    \begin{center}
        \caption{Ethernet PHY System Block Diagram\cite{Yamasaki2015}}
        \includegraphics[width=1.0\textwidth]{./Chapter2/Figures/EthernetPHYSystem.png}
    \label{EthernetPHYSystem}
    \end{center}
\end{figure}

\newpage
\section{Network Security}
As more applications rely on IoT edge-computing, it is critical that engineers ensure that computer networks still remain secure. Unfortunately, the more we protocols we implement, especially at the hardware-level, the more complicated our applications become; straining edge computers which are most efficient when they are only single-purpose (as seen in \cite{Isobe2010} and \cite{Restuccia2020}).

Additionally, it's not only network transfers where vulnerabilities can exist. Physical security measures must also be considered, as many embedded systems are deployed in accessible locations. Although unlikely, it is possible for dedicated hackers to create intrusions and RCEs based on memory-tampering \cite{Tsoutsos2018}. Such attacks have been around since the conception of the C programming language, Shao \etal shows how these attacks can be mitigated at a higher-level with intrusion detection \cite{Shao2003}, but ultimately, the onus is on developers to ensure that all low-level accesses to memory are consistently safe; \ie, there are no possible edge cases that can trigger buffer overflows \cite{Tsoutsos2018}.

We can address these issues, and still have robust security posture for networked embedded systems, however, to be implemented properly, regular reviews and updates are also required as the field of cybersecurity is constantly adapting to new vulnerabilities. Due to the vast scope, this thesis will only focus on secure transfers rather than memory protection.

\subsubsection{Transport Layer Security (TLS)}
Transport Layer Security (TLS) operates in an additional layer, the Presentation layer (OSI model), between the Transport and Application layers of the TCP/IP stack. As defined in RFC 8446 \cite{rfc8446}, TLS ensures three crucial security properties: confidentiality through encryption, integrity via Message Authentication Codes (MACs, not to be confused with Media Access Control (MAC)), and authentication using digital certificates.

The complete explanation of TLS 1.3 is out of scope for this thesis, but for a basic summary, TLS begins with a handshake phase where communicating parties negotiate protocol versions and cryptographic algorithms, authenticate identities, and establish shared secret keys. After this handshake, the TLS Record Protocol handles the bulk encryption of application data, breaking it into blocks, encrypting it with the negotiated algorithms, and adding integrity checks before transmission \cite{rfc8446}. Essentially, TLS ensures that no sensitive data is transferred without proper authorisation.

The computational overhead of TLS in IoT contexts has been extensively studied. Isobe \etal \cite{Isobe2010} demonstrated that even with dedicated FPGA acceleration, achieving 10Gbps throughput requires significant architectural considerations including parallel processing circuits, shared transmission/reception pathways, and optimized switch designs to reduce wire overhead. Their implementation, while achieving high performance (10Gbps), required significant FPGA resources - approximately 48,000 slices for the complete TLS stack and 23W of constant power. Keep in mind as well, this work from 2010 predates modern TLS 1.3, which introduces its own set of performance considerations and security limitations \cite{rfc8446}.

For resource-constrained IoT devices, Restuccia et al. \cite{Restuccia2020} revealed that the choice of cryptographic operations drastically impacts energy consumption. Their measurements showed that while TLS with Pre-Shared Keys (PSK) and AES-128-CCM consumed only 2.3 millicoulombs of charge during a complete handshake, upgrading to Elliptic Curve Diffie-Hellman (ECDHE) with ECDSA increased this to 63.4 millicoulombs - a factor of nearly 28x. They also demonstrated that moving from AES-128-CCM to AES-256-GCM increased memory requirements significantly, with heap usage growing from 5.7KB to 20.6KB for TLS 1.2 \cite{Restuccia2020}.

To summarise, the choice of cryptographic operations significantly impacts resource utilization and power used, especially when implementing hardware-accelerated AES operations \cite{Restuccia2020}. These implementation decisions become crucial when deploying TLS in resource-constrained environments where computational capabilities, memory, and power consumption must be carefully balanced against security requirements.

\newpage
\subsection{AES Encryption}
This thesis will focus on efficiently implementing one such security consideration, albeit a critical one, which is encryption. Specifically, we will be implementing the Advanced Encryption Standard (AES), which has become the de-facto standard for symmetric key encryption in computer security.

At its core, AES is a block cipher that operates on fixed-size blocks of data (128 bits) using cipher keys of 128, 192, or 256 bits. The encryption process consists of multiple rounds of several processing steps, all of which aim to obfuscate the input data, yet also maintain reversibility for decryption. Here is a high-level overview of the specific steps to perform an AES encryption, paraphrased from the \textit{Wikipedia} article, \cite{Wikipedia_AES_2024}. \\ \\
1. $KeyExpansion$ â€“ round keys are derived from the cipher key. AES uses a separate 128-bit round

key block for each round plus one more. \\
2. $AddRoundKey$ - each byte of the state is combined with a byte of the round key using bitwise

XOR. This function is responsible for incorporating the key into encryption process.

\begin{figure}[!htbp]
    \centering
    \begin{minipage}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Chapter2/Figures/aes_round_function.png}
        \caption{AES Round Function \cite{Wikipedia_AES_2024}}
        \label{fig:aes_round}
    \end{minipage}
    \hfill
    \begin{minipage}{0.5\textwidth}
        \text{3. Then the middle rounds are executed:}
        \begin{enumerate}
            \item $SubBytes$: each byte is substituted with another according to a lookup table (the SBox).
            \item $ShiftRows$: then, the last three rows of the substituted bytes are shifted cyclically.
            \item $MixColumns$: The four bytes in each column are then combined.
            \item $AddRoundKey$: finally, the round key is XOR-ed with each byte of the state (same as step 2).
        \end{enumerate}
        \vspace{0.5em}
        These middle rounds are executed $n$ times, depending on the key size:
        \begin{itemize}
            \item $n=10$ rounds for 128-bit keys
            \item $n=12$ rounds for 192-bit keys
            \item $n=14$ rounds for 256-bit keys
        \end{itemize}
    \end{minipage}
\end{figure}

The final output is our encrypted input text. AES has been heavily and publicly scrutinised for decades since its adoption by the U.S. government \cite{FIPS197} (aside from side-channel attacks, not technically a fault of the algorithm). It's also performant, as it can be implemented in-place with constant-time operations. And it's flexible, as multiple key-sizes are available and can be chosen based on an application's memory/time complexity constraints.

It's worth mentioning too that the decryption process essentially reverses the encryption process; only it uses inverse versions of the encryption steps, applied in a reverse order.

\newpage
\subsection{Performing an AES encryption via OpenSSL}
A single AES 256-bit CBC encryption of a text file can be performed via the open-source, OpenSSL library which implements SSL and TLS protocols for linux systems:

\texttt{
```
openssl enc -aes-256-cbc -salt -in plaintext.txt -out encrypted.bin -k \\ MySecretKey -pbkdf2
'''
}

Where:
\begin{itemize}
    \item \texttt{-aes-256-cbc}: Specifies the encryption algorithm (AES-256 in CBC mode).
    \item \texttt{-salt}: Adds a salt to the password for added security.
    \item \texttt{-in plaintext.txt}: The input file to be encrypted.
    \item \texttt{-out encrypted.bin}: The output file where the encrypted data will be stored.
    \item \texttt{-k MySecretKey}: The key used for encryption.
    \item \texttt{-pbkdf2}: Uses PBKDF2 (Password-Based Key Derivation Function 2) for key generation, which is more secure than the default (A warning for deprecated key generation usage will appear if this is omitted (OpenSSL 3.3.1)).
\end{itemize}
The `\texttt{plaintext.txt}' file is only securely encrypted if the key is not made public.
\section{RISC-V Instruction Set Architecture}
RISC-V is an open-source Instruction Set Architecture (ISA) developed at the University of California, Berkeley in 2010 [1]. It aims to provide a modern, modular, and extensible ISA for a wide range of computing devices. The RISC-V ecosystem has grown rapidly, with major tech companies and startups adopting the architecture. As of 2024, RISC-V processors have made substantial progress in performance, though they still lag behind the most advanced x86 and ARM designs in high-performance applications. For instance, the SiFive Performance P670 core, announced in late 2022, claimed performance comparable to Arm Cortex-A77 cores from 2019 [2].

RISC-V's main advantage, is in it's open-sourceness and potential for customization. Many companies are leveraging RISC-V for specialized applications, such as AI accelerators, where the ability to add custom instructions can provide significant advantages [3]. While RISC-V is still maturing in the high-performance computing space, it has already found significant success in microcontrollers and embedded systems. As of 2023, RISC-V International reported that over 10 billion RISC-V cores have been shipped, primarily in microcontrollers and embedded devices [4]. As the ecosystem continues to evolve, with more software support and advanced implementations, RISC-V is poised to become an increasingly important player in the processor market across various computing domains.

\section{FPGAs}
\subsection{Softcore CPUs}

\subsection{Chosen Softcore CPU: VexRiscv}

\subsection{SpinalHDL}

\subsection{CPU Buses, Wishbone}

\subsection{VexRiscvSMP}

\section{Litex}

\subsection{Cores}

\subsection{Litex BIOS}

\subsection{Buildroot Linux}
\raggedbottom
